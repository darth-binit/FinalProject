{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-20T17:55:01.454097Z",
     "start_time": "2025-03-20T17:55:01.445071Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import tqdm"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparing the Dataset",
   "id": "7932f42ffa989fa1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "# Define transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1,1] range\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])"
   ],
   "id": "1e81b4eec5db2cbe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T17:51:03.502208Z",
     "start_time": "2025-03-20T17:51:03.500276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TRAIN_DATA_PATH = \"/path/to/your/dataset\"\n",
    "TEST_DATA_PATH = \"/Users/binit/PycharmProjects/FinalProject/Project_File/data\"\n",
    "# Load dataset\n",
    "train_dataset = datasets.ImageFolder(root=TRAIN_DATA_PATH + \"/train\", transform=train_transforms)\n",
    "test_dataset = datasets.ImageFolder(root=TEST_DATA_PATH + \"/test\", transform=test_transforms)\n",
    "\n",
    "# Create DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Print class names\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "NUM_CLASSES = len(train_dataset.classes)"
   ],
   "id": "6d8c2997018841fd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Building",
   "id": "c88ba8aef15a6c01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T17:51:23.362005Z",
     "start_time": "2025-03-20T17:51:23.357584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CBAM(nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels):\n",
    "    super(CBAM, self).__init__()\n",
    "    self.channel_attention = nn.Sequential( ## this is for channel\n",
    "        nn.AdaptiveAvgPool2d(1),   ## input (32, 128, 8, 8) --> output (32, 128, 1, 1)\n",
    "        nn.Conv2d(in_channels, in_channels //8, 1), ## input as above , output as (32, 16, 1, 1)\n",
    "        nn.ReLU(),       ## output same\n",
    "        nn.Conv2d(in_channels // 8, in_channels, 1),   ##input as above --> output (32, 128, 1, 1)\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    self.spatial_attention = nn.Sequential(nn.Conv2d(2, 1, kernel_size=7, padding=3), ## this is for spatial\n",
    "                                                     nn.Sigmoid())\n",
    "\n",
    "  def forward(self, x):\n",
    "    # channel attention\n",
    "    x_channel = self.channel_attention(x) * x  ## output --> (32, 128, 8,8)\n",
    "\n",
    "    # spatial attention\n",
    "    avg_out = torch.mean(x_channel, dim=1, keepdim=True) ## output --> (32, 1,8, 8)\n",
    "    max_out, _ = torch.max(x_channel, dim=1, keepdim=True) ##output --> (32, 1, 8, 8)\n",
    "    x_spatial = torch.cat([avg_out, max_out], dim =1) ##output --> (32, 2, 8, 8)\n",
    "    spatial_attention = self.spatial_attention(x_spatial) ##output --> (32, 1, 8, 8)\n",
    "\n",
    "    return x_channel * spatial_attention ##(32, 128, 8, 8) * (32, 1, 8, 8) | output will be (32, 128, 8, 8)"
   ],
   "id": "80af72b72965baea",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T17:51:27.330057Z",
     "start_time": "2025-03-20T17:51:27.325954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels):\n",
    "    super(SelfAttention, self).__init__()\n",
    "    self.query = nn.Conv2d(in_channels, in_channels//8, 1) ## output (32, 16, ,8, 8)\n",
    "    self.keys = nn.Conv2d(in_channels, in_channels//8, 1)\n",
    "    self.value = nn.Conv2d(in_channels, in_channels, 1) ## output (32, 128, 8, 8)\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch, channels, height, width = x.size() ## input (32, 128, 8, 8)\n",
    "\n",
    "    query = self.query(x).view(batch, -1, width*height) ## input (32, 128, 8, 8) --> self.query--> output(32, 16,8,8)--> output (32,16,64)\n",
    "    key = self.keys(x).view(batch, -1, width*height) ## output (32, 16, 64)\n",
    "\n",
    "    attention = self.softmax(torch.bmm(key.permute(0, 2, 1), query))  ## output --> (32, 64, 64)\n",
    "\n",
    "    value = self.value(x).view(batch, -1, width * height) ##  (32, 128, 64) @ (32, 64, 64) --> output ( 32, 128, 64)\n",
    "    out  = torch.bmm(value, attention.permute(0,2,1)).view(batch, channels, height, width)\n",
    "    ## (32, 128, 64) @ (32, 64, 64) --> output ( 32, 128, 64) again we view it so final_output (32, 128, 8,8)\n",
    "\n",
    "    return out + x"
   ],
   "id": "f63c2bf48e956e4b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T17:51:54.591187Z",
     "start_time": "2025-03-20T17:51:54.586096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNN_Attention(nn.Module):\n",
    "\n",
    "  def __init__(self, num_classes=7):\n",
    "    super(CNN_Attention, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1) ##output --> (32, 32, ,32, 32) -> batch_size, 32 will be output/i.e. 32 features extracted images or channel increased from 3 to 32/ 32 x 32 is h,w\n",
    "    self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) ##output --> (32, 64, 32, 32) -> batch_size, channels increased from 32 to 64, h, w\n",
    "    self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) ##output --> (32, 128, 32, 32)\n",
    "    self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    # self.se_block = SqueezeExcite(128) ## this can be redundant as CBAM already includes this\n",
    "    self.cbam_block = CBAM(128)\n",
    "    self.attention_block = SelfAttention(128)\n",
    "\n",
    "    self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "    self.dropout = nn.Dropout(0.5)\n",
    "    self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.bn1(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.bn2(self.conv2(x)))) ##conv2 output is (32,64,32,32) --> when it is goes to pool output is --> (32, 64, 16, 16)\n",
    "    x = self.pool(F.relu(self.bn3(self.conv3(x))))  ##input (32,64, 16,16) --> conv3 -->output (32, 128, 16, 16) --> pool output -->((32, 128, 8, 8))\n",
    "\n",
    "    # x = self.se_block(x) ## Applying SE attention\n",
    "    x = self.cbam_block(x) # Applying CBAM attention ##input->(32,128, 8, 8). | output --> (32, 128,8, 8)\n",
    "\n",
    "    x = self.attention_block(x) ##output --> (32, 128, 8, 8)\n",
    "\n",
    "    x = self.global_avg_pool(x).view(x.size(0), -1)\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    x = self.fc(x)\n",
    "\n",
    "    return x"
   ],
   "id": "82ed271e1811b2c8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T17:52:46.614643Z",
     "start_time": "2025-03-20T17:52:46.607865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Intialize Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN_Attention(num_classes=10).to(device)\n",
    "\n",
    "# Loss Function, since its a mutliclass\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Learning Rate Scheduler (Optional)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ],
   "id": "4676e434eff7b71f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 119965\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "fe68a7fdb4af811e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T17:55:50.255272Z",
     "start_time": "2025-03-20T17:55:50.247545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Intialize Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN_Attention(num_classes=10).to(device)\n",
    "\n",
    "# Loss Function, since its a mutliclass\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Learning Rate Scheduler (Optional)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ],
   "id": "38b80c469b24311a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 119965\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T17:55:14.258751Z",
     "start_time": "2025-03-20T17:55:14.235553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    ###### TRAINING PHASE ######\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix(loss=running_loss / len(train_loader), acc=100 * correct / total)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    ###### VALIDATION PHASE ######\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Track validation statistics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "\n",
    "    ###### PRINT EPOCH SUMMARY ######\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} -> \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    scheduler.step()  # Update Learning Rate"
   ],
   "id": "47779f7f6d3668b6",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      7\u001B[39m correct = \u001B[32m0\u001B[39m\n\u001B[32m      8\u001B[39m total = \u001B[32m0\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m progress_bar = tqdm(\u001B[43mtrain_loader\u001B[49m, desc=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mEPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m, leave=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m progress_bar:\n\u001B[32m     13\u001B[39m     images, labels = images.to(device), labels.to(device)\n",
      "\u001B[31mNameError\u001B[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Validation & Evaluation",
   "id": "339fecc9df793974"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_total = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_total += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {loss_total/len(dataloader):.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Run evaluation on test dataset\n",
    "evaluate(model, test_loader)"
   ],
   "id": "5b2b0339e2f1b183"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualization of Training",
   "id": "e3b3204948fa8602"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def show_cam_on_image(img, mask):\n",
    "    \"\"\" Overlay the CAM heatmap on the image \"\"\"\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam\n",
    "\n",
    "def get_grad_cam(model, image, class_idx):\n",
    "    \"\"\" Compute Grad-CAM heatmap \"\"\"\n",
    "    model.eval()\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(image)\n",
    "    model.zero_grad()\n",
    "    output[0, class_idx].backward()\n",
    "\n",
    "    # Get gradients\n",
    "    gradients = model.conv3.weight.grad.cpu().data.numpy()\n",
    "    activations = model.conv3(image).cpu().data.numpy()\n",
    "\n",
    "    # Compute Grad-CAM\n",
    "    weights = np.mean(gradients, axis=(2, 3))\n",
    "    cam = np.sum(weights[:, :, None, None] * activations, axis=1)\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / np.max(cam)\n",
    "\n",
    "    return cam\n",
    "\n",
    "# Select a test image\n",
    "sample_image, sample_label = test_dataset[10]\n",
    "cam = get_grad_cam(model, sample_image, sample_label)\n",
    "\n",
    "# Display\n",
    "plt.imshow(show_cam_on_image(sample_image.permute(1, 2, 0).numpy(), cam[0]), cmap='jet')\n",
    "plt.show()"
   ],
   "id": "575b22be0b60898b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
